\documentclass[
12pt,
a4paper,
oneside,
headinclude,
footinclude]{article}

\usepackage[table,xcdraw,svgnames, dvipsnames]{xcolor}
\usepackage[capposition=bottom]{floatrow}
\usepackage[colorlinks]{hyperref} % to add hyperlinks
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{csquotes}
\usepackage{amsmath} % For the big bracket
\usepackage[export]{adjustbox}[2011/08/13]
% \usepackage{subfig}
\usepackage{array}
\usepackage{url}
\usepackage{graphicx} % to insert images
\usepackage{titlepic} % to insert image on front page
\usepackage{geometry} % to define margin
\usepackage{listings} % to add code
\usepackage{caption}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage{color}
\usepackage[nochapters, dottedtoc]{classicthesis}
\usepackage{listings} % For Python code
\usepackage{float}
\usepackage[caption = false]{subfig} % For 2x2 grid of images

\usepackage[ruled]{algorithm2e} % For pseudo-code

\usepackage{mathpazo}

\usepackage{amsthm} % For definitions and theorems

\theoremstyle{definition} % Define the style of definitions
\newtheorem{definition}{Definition}[section]


\usepackage{lipsum} % For testing
\usepackage{color}

\usepackage{etoolbox}

\usepackage{bm} % For bold math

\usepackage{setspace}


% For tables
\usepackage{amssymb}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}


\definecolor{webbrown}{rgb}{.6,0,0}

\usepackage{titlesec} % to customize titles
\titleformat{\chapter}{\normalfont\huge}{\textbf{\thechapter.}}{20pt}{\huge\textbf}[\vspace{2ex}\titlerule] % to customize chapter title aspect
\titleformat{\section} % to customize section titles
{\fontsize{14}{15}\bfseries}{\thesection}{1em}{}

\titlespacing*{\chapter}{0pt}{-50pt}{20pt} % to customize chapter title space

\graphicspath{ {../Figures/} } % images folder
\parindent0pt \parskip10pt % make block paragraphs
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm,footskip=1cm} % define margin
\hyphenation{Fortran hy-phen-ation}

\AtBeginDocument{%
	\hypersetup{
		colorlinks=true, breaklinks=true, bookmarks=true,
		urlcolor=webbrown, citecolor=Black, linkcolor=Black% Link colors
}}

\pagestyle{plain}
\title{\textbf{NLP Assignment 3 \\ Sentiment Analysis}}
\author{{Alberto Parravicini}}
\date{}	% default \today

% =============================================== BEGIN


\begin{document}
\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

\section{Introduction}
\textbf{Assignment weight: $3$}

\textit{Full code and data available at\\ \href{https://github.com/AlbertoParravicini/nlp-ulb/tree/master/assignment-3}{https://github.com/AlbertoParravicini/nlp-ulb/tree/master/assignment-3}}

The goal of the assigment is to experiment with different techniques used for sentiment analysis, and compare the results obtained. \\
As a starting point, it was used the dataset of \textbf{Digital Music} reviews on \textbf{Amazon}, compiled by \textbf{Julian McAuley}. The goal was to predict the score given to a product by analysing its review, by taking advantage of \textit{Natural Language Processing} and \textit{Machine Learning}.\\
This report will present various preprocessing and modelling techniques that have been tried, such as \textbf{Latent Semantic Analysis} and \textbf{Support Vector Machines},
and discuss their efficiency.

The first section of the report will detail the data that have been used, and the preprocessing techniques applied to them.\\
The second section is focused on the models that were used for sentiment analysis, and on the selection and validation techniques that have been adopted.
The third and last section will present the results of the models, and discuss problems and potential improvements that can be adopted.

\newpage

\textit{Note:} instead of keeping all the code in a single file, I preferred to use multiple files: one takes care of the preprocessing, while the others contain the models used for the predictions. 

\begin{itemize}
    \item \texttt{preprocess.py} is used to pre-process the original dataset and create different embeddings/representations of the dataset, to be used in the predictions. These representations are stored as \texttt{hdf}, to save space. The size of the dataset makes them unsuitable to be added alongside this report. They are available at \href{https://github.com/AlbertoParravicini/nlp-ulb/tree/master/assignment-3}{https://github.com/AlbertoParravicini/nlp-ulb/tree/master/assignment-3}
    \item \texttt{naive\_bayes\_and\_svm.py} contains the models to be used with the occurrences matrices. \textbf{Naive Bayes} is suitable for the binary matrix, while \textbf{SVM} should be used with dense vectors. Refer to the specific section of the report for more informations.
    \item \texttt{sent\_scores.py} performs classification using the sentiment lexicon.
    \item \texttt{binary\_class.py} does binary classification instead of multi-class classification. 
\end{itemize}

\section{Data analysis and pre-processing}
The dataset used in the assignment is a collection of \textbf{Digital Music} reviews of songs and albums sold on \textbf{Amazon}. \\
Each review is stored as a \textbf{JSON}, with different fields such as the \textit{reviewer name}, the \textit{review date}, how many people found it \textit{useful}, and more.\\
Our goal is to process the \textbf{review text} (and the \textbf{review title}), in order to predict the review score. Scores range on a $1-5$ scale, and can be interpreted as the \textbf{sentiment} of the reviewer towards the product he has bought.\\
It should be noted that if our goal was to predict the scores as accurately as possible, then all the information in the review should be taken into account (such as the \textit{reviewer name}); however, our focus is on the text analysis, and we can discard those fields.

The dataset contains $64.706$ reviews, but we will use a smaller subset in order to reduce the computational costs of the algorithms, and to check whether using subsets of higher size can be beneficial.\\
Indeed, moving from a subset of $10000$ entries to a subset of $40000$ entries seems to positively impact the quality of the predictions. It can be assumed that using even more data could give further improvements, at the cost of greatly increased execution time of the training.
\vspace{15mm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, center, keepaspectratio=1]{{"figures/7"}.png}
    \caption{\emph{Distribution of the review scores in the dataset.}}
\end{figure}

\newpage
If we look at the distribution of the review scores in the dataset, it is clear how reviews with high scores are overrepresented.
People usually buy songs that they like, and it's possible to see how most negative reviews contains complaints that aren't about the song itself, but about the sound quality or mastering (\textit{"I love this song, but the sound quality is awful!"} is a very common pattern).  This has a few important implications:

\begin{itemize}
    \item Any model we use will have to consider the \textit{a-priori} probability of each class, as they are different.
    \item If our goal is to infer the sentiment of a sentence and not to predict the review score, it could be beneficial to build a new dataset in which every score has the same probability of appearing. However, this would heavily reduce the data at our disposal, and overall lead to worse results.
    \item Models based on regression are likely to perform badly, compared to multi-class classification models. Most regression models (such as any model based on linear regression) assume the output to be \textit{normally} (or at least symmetrically) distributed, while we have a highly skewed distribution. Models won't be able to accurately predict the values at the extremes of the distribution. On the other hand, multi-class classification will ignore the ordinal relation in the scores, hence using less information that they could.
    \item A trivial predictor that always give the majority class will have an accuracy of $54\%$ This value is our baseline, and will prove surprisingly hard to beat.
\end{itemize}

\subsection{Introduction to preprocessing}
A number of preprocessing techniques can be applied to the dataset, depending on what kind of approach we want to use to predict the scores. Different techniques have been adopted, and they are detailed below.

First, it was decided to work with a \textbf{unigram} model. For each review (from now on referred to as \textit{document}), we can count the number of occurrences of each word in the vocabulary. This will give a \textbf{term-document} matrix, which could be used as-is, or processed further.

To build a \textbf{term-document} matrix, there are a few things to keep into account.\\
First, we need to treat differently words that are in a \textbf{negated context}: it is obviously different to say that something is \textit{"good"} compared to being \textit{"not good"}, and we have to consider that when analysing the sentences. A common approach to deal with negated contexts is to append a prefix (e.g. \textbf{NOT\_}) to words that are between a negation and the following punctuation mark.\\
Instead, a more refined approach was taken: first, every document was analysed using the \textbf{spaCy} library for \textit{Python}. This will give the dependency tree of each sentence. Then, one can look for arcs labelled as \textbf{neg}, which denote a negation relationship: it is possible to append the \textbf{NOT\_} prefix to any word dependent on the head of the \textbf{neg} arc; this will mark the entire subordinate clause, which is more precise than the first basic approach that was presented. 

Then, it should be considered that the dataset contains a large amount of different words, and it is likely that not all of them will be useful in the prediction.\\
Punctuation signs, stop-words, and rare words will probably not be useful, and can be removed. To remove uncommon words, it is necessary to set an arbitrary threshold.\\
 Using a value of $20$ (meaning that words with less than $20$ occurrences were removed) seems to yield a good compromise between accuracy and dataset size.

Words were turned to lower-case: keeping words in upper-case could maybe be beneficial, as words written in upper-case have usually more \textit{"strength"} associated to them (think \textit{"bad"} vs. \textit{"BAD"}). The same applies to certain punctuation marks such as \textit{"!"}. Still, the increase in complexity caused by handling all these subtleties was deemed excessive for the \textbf{term-document} matrix.\\
It was also extracted the \textbf{lemma} of the words, instead of keeping them in their original form. Once again, we can assume that there is little difference between singular and plural nouns, or between different tenses.
Overall, the final vocabulary will contain about $10000$ terms.

From this processed dataset, it was built the \textbf{term-document} matrix (of size $|Documents| \times |Vocabulary|$), that lists for each document the number of occurrences of each word.\\
Note that instead of counting the number of occurrences, it was built a binary matrix, where a value of $1$ indicates that a certain word appears in the document. The idea is that repetitions of words won't be meaningful in the classification, compared to having or not a word. \\
To be more precise, both approaches were tried, with the binary matrix seeming sightly more powerful.\\

\subsection{Latent Semantic Analysis}

Note that the \textbf{term-document} matrix is very sparse, and rather difficult to handle. Moreover, binary features don't allow to represent the semantic of a word, or its importance in the document. Hence, it was decided to process further the binary matrix, by using \textbf{Latent Semantic Analysis}. \\
First, the occurrences matrix (not the binary one) was used to compute the \textbf{Tf-idf} values of each word, relative to each document. This matrix, also of size $|Documents| \times |Vocabulary|$, expresses the importance of each word, relatively to each document in which it appears.\\
Then, \textbf{Truncated SVD} was applied to the matrix, to reduce its dimensionality and obtain a dense representation of each document. Dimensions of $100$ and $500$ were tried, with the second value being sightly better.

Note that both the binary matrix and the dense matrix were kept, and tried with different models. The idea is that some models might prefer to work with dense numerical data, while others will prefer the binary values.\\
Moreover, some models such as \textbf{SVD} proved to be impossible to train on the binary matrix, due to its size. On the other hand, the dense matrix is more manageable, and could be used to train a larger amount of different models.

\subsection{Sentiment Lexicon}
Instead of simply looking at word occurrences, one can assign scores to words based on their meaning: \textit{"good"} and \textit{"excellent"} have both a positive connotation, but \textit{"excellent"} is definitely stronger than \textit{"good"}. On the other hand, a word such as \textit{"spaghetti"} will hardly denote a positive or negative meaning.\\
By computing the sentiment values of each word, it is possible to build more refined models to predict the review scores.

Sentiment values could be learnt from our dataset, in order to have values that are fine-tuned to the data at our disposal (for instance, a word such as \textit{"melody"} could have a more positive meaning than \textit{"song"}).\\
However, this approach is computationally intensive, and it's not guaranteed to provide good results, likely due to the small dataset at our disposal (compared to something like \textbf{WordNet}), which could lead to overfitting.\\
Instead, it is possible to use a pre-trained \textbf{Sentiment Lexicon}, which will significantly speed-up our work and is guaranteed to provide good results.

\textbf{NLTK} provides a pre-trained lexicon called \textbf{VADER} \cite{hutto2014vader}, which was used to score each review. For each review, it is possible to compute the \textbf{composite}, the \textbf{negative}, the \textbf{neutral} and \textbf{positive} scores. If one has to simply distinguish whether a review is positive or negative, it would be possible to simply look at the difference between positive and negative scores, and classify the document according to that. \\
However, if it is required to give a numerical classification, it can be beneficial to keep all $4$ values, and use them as features for some model (such as a \textbf{Multiclass Logistic Regression}).

\textbf{VADER} doesn't just score a sentence word-by-word, but looks at the relation of words, and makes use of negated contexts, punctuation, and adjectives. \\
As such, it can be useful to evaluate not only the sentences that were already processed in the first section (so, at the tokenized sentences with only the lemma, no punctuation marks, and more), but also at the original text. This will give $4$ more scores for each sentence, and they can also be used as features.

Below, the distribution of scores for each type and each review score, divided by pre-processed and original text.

\begin{figure}[H]
    \subfloat[Compund Score]{\includegraphics[width = 4in]{{"figures/2"}.png}}\
    \subfloat[Neutral Score]{\includegraphics[width = 4in]{{"figures/3"}.png}}\
    \label{sent1}
\end{figure}    
\begin{figure}[H]
    \subfloat[Negative Score]{\includegraphics[width = 4in]{{"figures/4"}.png}}\
    \subfloat[Positive Score]{\includegraphics[width = 4in]{{"figures/5"}.png}}\
    \caption{Distribution of sentiment scores based on review score.}
    \label{sent2}
\end{figure}

It can be noted how the sentiment scores reflect what we would expect: \textbf{negative} scores are associated to \textbf{negative} reviews, while \textbf{positive} scores are associated to \textbf{positive} reviews. \textbf{Neutral} scores show a very slight bump on the \textbf{average} scores, which is more noticeable in the pre-processed text.\\
In general, the distributions of pre-processed and unprocessed reviews are similar, but values are stronger in the pre-processed text, as most of the superfluous information was taken out.

\section{Model analysis}
After preprocessing the dataset, it is possible to train different models and see how they perform. As we have different representations of the same dataset (sparse matrix, dense matrix, and sentiment scores), we will also use different models that are best suited to work with each type of data.

$20\%$ of the dataset is used for validation; this set is built with \textit{stratified sampling}, to preserve the score distribution.\\
The remaining $80\%$ is used for training. Training is done through \textit{$10$-fold cross-validation}, to obtain a precise estimate of the model accuracy and find the best values of the hyper-parameters. \\
Then, the model is trained with the entire training set, and this model is tested on the validation set.
This approach is used for every model that has been considered.

\subsection{Multinomial Naive Bayes}
The first model to be considered is a \textbf{Multinomial Naive Bayes}. This model was applied to the sparse \textbf{term-document} matrix. In this model features are supposed to have a multinomial distribution, which can be assumed as entries of the input matrix are word counts. \\
Interestingly, using the binary occurrence matrix with a \textbf{Multinomial Naive Bayes} gives results that are better than both using the same model on the \textbf{term-document} matrix and of using a \textbf{Binomial Naive Bayes} on the binary occurrence matrix.

In the Naive Bayes model it is possible to set the value used for \textit{Laplace smoothing}: different values were tried and tested through cross-validation, and the best result seems to be around $3$.\\
Overall, the accuracy of the model is about $59\%$, just $5\%$ above the baseline. Still, this is the best model that was obtained. For a more in-depth discussion of the result, refer to the specific section.

The dense matrix obtained through \textbf{LSA} was used as input of an \textbf{SVM} classifier. Training the \textbf{SVM} on the sparse matrices proved computationally infeasible, while the training is rather fast on the dense matrix. \\
Still, the accuracy of this model is mediocre, at around $56\%$. However, using the dense matrix as input of the \textbf{Naive Bayes} gives even worse results (worse than the baseline): this could indicate that using a larger amount of dimensions, or even the sparse matrix, as input of the SVM could result in improvement on the best result obtained.

The dataset obtained from the sentiment lexicon was tested with different models, both for regression and classification (in the case of regression, prediction are rounded to the nearest integer): \textbf{Random Forest} regressors and classifiers, \textbf{Generalized Linear Models}, \textbf{Gaussian Naive Bayes}.\\
Overall, the best model seems to be a simple \textbf{Multiclass Logistic Regression}: even in this case, the results are mediocre, giving an accuracy of $56\%$.

This last result is rather surprising, considering that there is a strong correlation between the sentiment scores and the review scores, as shown in the previous plots.

\section{Result Analysis}
This section will focus on the analysis of the results obtained by the models. Specific attention will be given to the \textbf{Multinomial Bayes Model}, as it is the one that performed the best.

First, it is possible to see how many values of each type were predicted, versus the real ones.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth, center, keepaspectratio=1]{{"figures/6"}.png}
    \caption{\emph{Distribution of the predicted review scores, versus the real ones.}}
\end{figure}

It seems that the classifier overestimates the number of reviews with score $5$, at the expenses of negatively rated reviews. This could be caused by the disproportion of classes in the dataset, a further evidence that having a more balanced training set could give better results.\\
Still, increasing the size of the input dataset shows improvements: the \textbf{Multiclass Naive Bayes} goes from an accuracy of $56\%$ on a dataset with $10000$ reviews to $59\%$ if the dataset contains $40000$ reviews. With enough data, and computational power, it is likely that better results are achievable.\\
In any case, the results aren't very encouraging: if the bad results obtained by regression can be explained by the skewed distribution of the scores, the classifiers should perform much better. 

Using bigram counts would probably be very beneficial, but it is computationally infeasible given the available resources. It might be possible if the vocabulary was filtered further; however in this case the bigram would lose significance, as they would count words that aren't really close to each other. \\
Also, consider that with a vocabulary of just $1000$ words (down from the current $1000$), we would end up with $1000000$ possible bigrams. The real count would obviously be much lower, but still very hard to manage.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, center, keepaspectratio=1]{{"figures/8"}.png}
    \caption{\emph{Confusion matrix of the \textbf{Multiclass Naive Bayes} classifier.}}
\end{figure}

Most of the mistakes are done by predicting scores of value $4$ as having value $5$. Interestingly, there are also many mistakes in the opposite sense, which makes hard to say that our classifier is being too \textit{"generous"} with the scores it's giving.\\
There is also a certain tendency to assing $3$ instead of $2$: indeed, most reviews with score $2$ are actually classified as $3$.\\
Amusingly, there is a large set of reviews with score $5$ that are classified as $1$, and viceversa. 
We will look below at some of these mistakes.

If we pick the majority class, $5$, as reference point, it is also possible to compute other useful metrics.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, center, keepaspectratio=1]{{"figures/9"}.png}
    \caption{\emph{Confusion matrix of the \textbf{Multiclass Naive Bayes} classifier, for score $5$.}}
\end{figure}

We see once again how the number of predicted $5$ is much higher than the real amount. We can also measure:

$$accuracy = \frac{TP + TN}{TP + TN + FP + FN} = 0.68$$
$$balanced\ accuracy = \frac{1}{2}(\frac{TP}{TP + FN} + \frac{TN}{TN + FP}) = 0.65$$
$$sensitivity = \frac{TP}{TP + FN} = 0.8$$
$$specificity = \frac{TN}{TN + FP} = 0.5$$
$$precision = \frac{TP}{TP + FP} = 0.71$$

\begin{enumerate}
    \item \textbf{Balanced Accuracy} measures the accuracy taking into account the different class distributions, and has value $0.5$ in case the majority class is predicted. \\
    Here we see that the balanced accuracy is only sightly lower than the accuracy, which means that we are actually able to predict the class $5$, instead of just predicting the majority class.
    \item \textbf{Sensitivity} measures how many of the reviews with value $5$ were actually found. We can be satisfied with a value of $0.8$.
    \item \textbf{Specificity} measures the percentage of reviews with value lower than $5$ that were actually found: here the value is not very satisfactory, as it was seen how the classifier is overestimating the amount of $5$.
    \item \textbf{Precision} shows how many of the reviews that we classified as $5$ are actually $5$. Indeed, the value is not very high, as we predicted too many reviews to have value $5$.
\end{enumerate}

\subsection{Misclassification Analysis}
Below are reported snippets of $10$ misclassified examples. $5$ were predicted to have score $5$, but had a real score of $1$. The other $5$ are the opposite.

\begin{itemize}
    \item "$[\ldots]$ Mind you, Commin from Where I'm From is weak in comparison to some of my favorites on the CD like Lucille, Float, Charlene, and Mama Knew Love.  This Brother testifies on this CD and it is worth every dime.  Buy it and prep yourself for an experience."
    \item "$[\ldots]$ It's also not the first thing I'd buy in the store.While I can appreciate the effort, this record will largely appeal only to people for whom old Motown-type tracks from the 60s still hold sway. $[\ldots]$".
    \item "$[\ldots]$ A lot of the edge is taken out, some of the saxaphone solo isn't there, and it loses some of the darkness of the earlier version.""It Goes On,"" ""Dumb Waiters"", and ""Into You Like a Train"" have riffs boppy enough to dance to. $[\ldots]$".
    \item "$[\ldots]$ I have a problem with reviewers who probably have all of their favorite artist(s)' albums and go on record complaining that this song or that song wasn't on the ""greatest hits"" or ""best-of"" collection. I was a fan of Eddie Money's from the beginning, and was a little surprised to find that one reviewer thought his debut album was lame. $[\ldots]$".
    \item "$[\ldots]$ Track Listing. Side One1. Rock and Roll, Hoochie Koo2. Joy Ride [Instrumental]3. Teenage Queen4. Cheap Tequila5. Uncomplicated6. HoldSide Two1. Airport Giveth (The Airport Taketh Away)2. Teenage Love Affair3. It's Raining4. Time Warp [Instrumental]5. Slide on Over Slinky6. Jump, Jump, Jump,".
\end{itemize}

We see how in many cases there are problems due to minor complains that might be considered more important than they are. The last example is harder to interpret, as it's just a sequence of titles. Indeed, it can be seen that there are many reviews which don't really contain any coherent text, and it's not surprising that they are misclassified.

\begin{itemize}
    \item "A Great Album Deserves Better Sound. $[\ldots]$ But this album deserves to be heard with modern mastering. Anyone who cares about good sound, save your money, listen to your vinyl."
    \item "I Had No Idea. I kind of liked some of the songs these guys had on the radio. The one about Africa and Roseanna were cool songs, so I gave this a listen. $[\ldots]$."
    \item "Buy the SACD or the Blu-Ray. You want a ""deluxe"" edition? Go for superior quality and buy the SACD edition. It's incredible. CDs are low-fi crap no matter what you add to the song list."
    \item "Original master tape. Audio Fidelity does not use the Original master tapes on this and about all of the CD releases.  They claim the sources they use are going to give the consumer a great sounding CD.If you're not using the Original master tape then you're comparing HDTV to analog TV.This version sounds very slightly better then the Motown release but compare the cost.I wish Mobile Fidleity would release this because they use nothing but the Original Master tape and the sound comes through.A great Stevie Wonder CD, but all of his CD's are great!!!!!!!!!!!!!!!!!"
    \item "Not his best, but a very attractive. Almost any CD from this gifted singer, is a delight for your ears! ""L Is For Lover"", not being one of his best, is certainly an excellent addition to your Male Jazz/Pop singers collection. I don't know exactly why there is now a ""shortage"" of Al Jarreau's CDs in the market (perhaps it is due to that usually stupid fight between record companies for copyrights \&/or royalties), but if you have the chance to get it, do it right now! (I had to buy mine as a used one -a German issue, the sound quality is outstanding!)."    
\end{itemize}

In the case of negative reviews classified as positive, it can be seen how words with positive connotation are often used to describe other songs or artist, but are used nonetheless to mark the review as positive.\\
The last $2$ cases seem to be human error, as the reviews contain a positive description, followed by an extremely negative score.




\bibliographystyle{plainurl}
\bibliography{bibliography}

\end{document}