\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{lof}{\deactivateaddvspace  }
\@writefile{lot}{\deactivateaddvspace  }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{i}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction to character-level N-grams models}{i}{section.2}}
\citation{cavnar1994n}
\@writefile{toc}{\contentsline {section}{\numberline {3}Text Preprocessing}{iii}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}3-Grams Model Building}{iv}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \emph  {Sparsity patterns of the occurrences matrices, for the British English corpus. In each matrix are visualized the non-zero occurring trigrams ending with a given letter.}\relax }}{v}{figure.caption.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of occurrences of common suffixes across the 3 English variety.\relax }}{vi}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:table-occ}{{1}{vi}{Number of occurrences of common suffixes across the 3 English variety.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Text Generation}{vi}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Text Classification}{vii}{section.6}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces log-Perplexity of the various sentences (shown here before preprocessing), tested on the 3 models. Numbers in \leavevmode {\color  {MidnightBlue}blue} are correctly predicted labels. Numbers in \leavevmode {\color  {BrickRed}red} are wrongly predicted labels.\relax }}{viii}{table.caption.3}}
\newlabel{tab:table-perplexity}{{2}{viii}{log-Perplexity of the various sentences (shown here before preprocessing), tested on the 3 models. Numbers in \textcolor {MidnightBlue}{blue} are correctly predicted labels. Numbers in \textcolor {BrickRed}{red} are wrongly predicted labels.\relax }{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Confusion Matrix of the British English variety.\relax }}{viii}{table.caption.4}}
\newlabel{tab:table-conf-gb}{{3}{viii}{Confusion Matrix of the British English variety.\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Confusion Matrix of the American English variety.\relax }}{viii}{table.caption.5}}
\newlabel{tab:table-conf-us}{{4}{viii}{Confusion Matrix of the American English variety.\relax }{table.caption.5}{}}
\citation{kanaris2007words}
\citation{frasconi2002hidden}
\citation{lai2015recurrent}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Confusion Matrix of the Australian English variety.\relax }}{ix}{table.caption.6}}
\newlabel{tab:table-conf-au}{{5}{ix}{Confusion Matrix of the Australian English variety.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{ix}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Annex: Python Code}{x}{section.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Model Building}{x}{subsection.8.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Text Generation}{xii}{subsection.8.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Text Classification}{xiii}{subsection.8.3}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Annex: Language Model Extract}{xv}{section.9}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Probabilities of trigrams starting in \textbf  {iz} in British English.\relax }}{xv}{table.caption.7}}
\newlabel{tab:table-prob-gb}{{6}{xv}{Probabilities of trigrams starting in \textbf {iz} in British English.\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Probabilities of trigrams starting in \textbf  {iz} in American English.\relax }}{xvi}{table.caption.8}}
\newlabel{tab:table-prob-us}{{7}{xvi}{Probabilities of trigrams starting in \textbf {iz} in American English.\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Addendum: Kneser-Ney Smoothing}{xvi}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Snippets of the implementation}{xvii}{subsection.10.1}}
\bibstyle{plainurl}
\bibdata{bibliography}
\bibcite{cavnar1994n}{1}
\bibcite{frasconi2002hidden}{2}
\bibcite{kanaris2007words}{3}
\bibcite{lai2015recurrent}{4}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  55CD3F91388B0D293458FC3A6F43DE0B7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  AC2D76E753BE4A25E7B94ABBCE420CB97C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  D52756737E4CFE0112C6E2AABE47005A7C2E7F49E087B8E8E8DF5244CE287E39.pygtex,
  36AB0560E2CA3FC7074610E3F934F9397C2E7F49E087B8E8E8DF5244CE287E39.pygtex}
